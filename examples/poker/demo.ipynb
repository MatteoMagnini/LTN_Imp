{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltn_imp.automation.knowledge_base import KnowledgeBase\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt.pyll.base import scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "poker_hand = fetch_ucirepo(id=158)   \n",
    "X = poker_hand.data.features \n",
    "y = poker_hand.data.targets \n",
    "data = pd.DataFrame(X, columns=poker_hand.data.feature_names)\n",
    "data[\"Label\"] = y\n",
    "\n",
    "def prepare_datasets(data, random_seed=42):\n",
    "\n",
    "    def limit_class_instances(data, label_column, max_instances=50000):\n",
    "        return data.groupby(label_column).apply(lambda x: x.sample(n=min(len(x), max_instances), random_state=random_seed)).reset_index(drop=True)\n",
    "    \n",
    "    # Apply the limit to the dataset\n",
    "    data_limited = limit_class_instances(data, label_column=\"Label\", max_instances=2000)\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = data_limited.drop(\"Label\", axis=1)  # Features\n",
    "    y = data_limited[\"Label\"]  # Labels\n",
    "\n",
    "    # Split into train, test, and validation sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=random_seed, stratify=y_train)\n",
    "\n",
    "    # Combine features and labels for train, validation, and test sets\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    val_data = pd.concat([X_val, y_val], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    # Save to CSV files\n",
    "    train_data.to_csv(\"datasets/train.csv\", index=False)\n",
    "    val_data.to_csv(\"datasets/val.csv\", index=False)\n",
    "    test_data.to_csv(\"datasets/test.csv\", index=False)\n",
    "\n",
    "    return data_limited, train_data, val_data, test_data\n",
    "\n",
    "data, train, val, test = prepare_datasets(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    2000\n",
       "1    2000\n",
       "2    2000\n",
       "3    2000\n",
       "4    2000\n",
       "5    2000\n",
       "6    1460\n",
       "7     236\n",
       "8      17\n",
       "9       8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts(\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def find_best_models(X,y):\n",
    "    param_grid = {\n",
    "        'KNN': {\n",
    "            'model': KNeighborsClassifier(),\n",
    "            'params': {\n",
    "                'n_neighbors': [3, 5, 7],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'p': [1, 2]\n",
    "            }\n",
    "        },\n",
    "        'DT': {\n",
    "            'model': DecisionTreeClassifier(),\n",
    "            'params': {\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'criterion': ['gini', 'entropy']\n",
    "            }\n",
    "        },\n",
    "        'RF': {\n",
    "            'model': RandomForestClassifier(),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'criterion': ['gini', 'entropy']\n",
    "            }\n",
    "        },\n",
    "        'LR': {\n",
    "            'model': LogisticRegression(max_iter=1000),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'solver': ['lbfgs', 'liblinear']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    for name, model_info in param_grid.items():\n",
    "        grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X, y)\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "\n",
    "    models = []\n",
    "    for name, model in best_models.items():\n",
    "        if name == 'KNN':\n",
    "            models.append(KNeighborsClassifier(**model.get_params()))\n",
    "        elif name == 'DT':\n",
    "            models.append(DecisionTreeClassifier(**model.get_params()))\n",
    "        elif name == 'RF':\n",
    "            models.append(RandomForestClassifier(**model.get_params()))\n",
    "        elif name == 'LR':\n",
    "            models.append(LogisticRegression(**model.get_params()))\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = find_best_models(train.drop(\"Label\", axis=1), train[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def save_models(models, filenames):\n",
    "    for model, filename in zip(models, filenames):\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "save_models(models, [\"models/KNN.pkl\", \"models/DT.pkl\", \"models/RF.pkl\", \"models/LR.pkl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, dropout_prob=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        \n",
    "        # Build layers with dropout\n",
    "        for out_size in hidden_layers:\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.LeakyReLU())  # LeakyReLU activation\n",
    "            layers.append(nn.Dropout(p=dropout_prob))  # Dropout layer\n",
    "            in_size = out_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "100%|██████████| 50/50 [1:04:40<00:00, 77.60s/trial, best loss: 1.5605933666229248]\n",
      "Best hyperparameters: {'dropout_prob': np.float64(0.19408016570692213), 'hidden_layer_sizes': np.int64(4), 'learning_rate': np.float64(0.016817041194092816), 'num_epochs': np.float64(23.0), 'weight_decay': np.float64(0.002487585739865293)}\n"
     ]
    }
   ],
   "source": [
    "kb = KnowledgeBase(\"config.yaml\")\n",
    "\n",
    "def objective(params):\n",
    "    learning_rate = params['learning_rate']\n",
    "    weight_decay = params['weight_decay']\n",
    "    hidden_layer_sizes = params['hidden_layer_sizes']\n",
    "    num_epochs = params['num_epochs']\n",
    "    dropout_prob = params['dropout_prob']  # Add dropout probability\n",
    "\n",
    "    # Initialize the model with the dropout probability\n",
    "    model = MLP(input_size=10, output_size=10, hidden_layers=hidden_layer_sizes, dropout_prob=dropout_prob)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(num_epochs):  # Use num_epochs here\n",
    "        for batch_X, batch_y in kb.loaders[0]:  # Assuming kb.loaders[0] is train_loader\n",
    "            batch_y = batch_y.view(-1).long()  # Ensure correct shape and type\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in kb.test_loaders[0]:  # Assuming kb.test_loaders[0] is test_loader\n",
    "            batch_y = batch_y.view(-1).long()  # Ensure correct shape and type\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),  # 1e-5 to 1\n",
    "    'weight_decay': hp.loguniform('weight_decay', -6, -2),   # 1e-6 to 1e-2\n",
    "    'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [\n",
    "    [256, 128, 64, 32, 16],   # Architecture with 5 layers\n",
    "    [128, 64, 32, 16],        # Architecture with 4 layers\n",
    "    [256, 128, 64],           # Architecture with 3 layers\n",
    "    [128, 64],                # Architecture with 2 layers\n",
    "    [256],                    # Architecture with 1 layer\n",
    "]),\n",
    "    'num_epochs': scope.int(hp.quniform('num_epochs', 5, 50, 1)),  # Epochs range from 5 to 50\n",
    "    'dropout_prob': hp.uniform('dropout_prob', 0.1, 0.5)  # Dropout probability between 0.1 and 0.5\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,  # Use the TPE algorithm\n",
    "    max_evals=50,  # Number of evaluations\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(filenames):\n",
    "    models = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'rb') as file:\n",
    "            models.append(pickle.load(file))\n",
    "    return models\n",
    "\n",
    "models = load_models([\"models/KNN.pkl\", \"models/DT.pkl\", \"models/RF.pkl\", \"models/LR.pkl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBase(\"config.yaml\")\n",
    "models.append(kb.predicates[\"PokerHand\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01734952357068697\n",
    "weight_decay = 0.0033271235322686003\n",
    "epochs = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, device):\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Iterate over the data loader\n",
    "    for data, labels in loader:\n",
    "        # Move data and labels to the specified device\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Get predictions from the model\n",
    "        if isinstance(model, torch.nn.Module):            \n",
    "            with torch.no_grad():\n",
    "                predictions = model(data)\n",
    "                predicted_labels = torch.argmax(predictions, dim=1)\n",
    "        else:\n",
    "            predicted_labels = model.predict(data)\n",
    "\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        all_labels.extend(labels.cpu().numpy())  \n",
    "        all_predictions.extend(predicted_labels.cpu().numpy()) \n",
    "    else:\n",
    "        all_labels.extend(labels)\n",
    "        all_predictions.extend(predicted_labels)\n",
    "\n",
    "    if isinstance(model, torch.nn.Module):\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_predictions = np.array(all_predictions)\n",
    "\n",
    "    # Compute sklearn metrics\n",
    "    overall_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro') \n",
    "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    return overall_accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_df, metrics_to_plot):\n",
    "    plt.figure(figsize=(18, len(metrics_to_plot) * 4))\n",
    "\n",
    "    for i, metric in enumerate(metrics_to_plot, 1):\n",
    "        plt.subplot(len(metrics_to_plot), 1, i)\n",
    "        data_to_plot = [metrics_df.loc[model_name, metric] for model_name in metrics_df.index]\n",
    "        \n",
    "        # Customize the boxplots\n",
    "        boxprops = dict(linewidth=2)\n",
    "        medianprops = dict(linewidth=2, color='red')\n",
    "        meanprops = dict(linewidth=2, color='blue')\n",
    "        whiskerprops = dict(linewidth=2)\n",
    "        capprops = dict(linewidth=2)\n",
    "        \n",
    "        plt.boxplot(data_to_plot, labels=metrics_df.index, boxprops=boxprops, \n",
    "                    medianprops=medianprops, meanline=True, showmeans=True, \n",
    "                    meanprops=meanprops, whiskerprops=whiskerprops, \n",
    "                    capprops=capprops)\n",
    "        \n",
    "        plt.title(f'Boxplot of {metric} across different models')\n",
    "        plt.xlabel('Model', fontweight='bold')\n",
    "        plt.ylabel(metric, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "def compare_models(metrics_df):\n",
    "\n",
    "    best_models = {}\n",
    "    for metric in metrics_df.columns:\n",
    "        try:\n",
    "            mean_scores = metrics_df[metric].apply(np.mean)\n",
    "            best_model = mean_scores.idxmax()  \n",
    "            best_models[metric] = best_model\n",
    "        except:\n",
    "            print(f\"Rule {metric} not available\")\n",
    "            continue\n",
    "\n",
    "    results = {}\n",
    "    for metric in metrics_df.columns:\n",
    "        try:   \n",
    "            best_model = best_models[metric]\n",
    "            best_scores = metrics_df.loc[best_model, metric]\n",
    "            \n",
    "            results[metric] = {}\n",
    "            \n",
    "            for model_name in metrics_df.index:\n",
    "                if model_name == best_model:\n",
    "                    continue\n",
    "                \n",
    "                comparison_scores = metrics_df.loc[model_name, metric]\n",
    "                t_stat, p_value = stats.ttest_rel(best_scores, comparison_scores)\n",
    "                results[metric][model_name] = p_value \n",
    "\n",
    "        except:\n",
    "            print(f\"Rule {metric} not available\")\n",
    "            continue\n",
    "\n",
    "    significance_level = 0.05\n",
    "    for metric, comparisons in results.items():\n",
    "        try:\n",
    "            print(f\"\\n{metric}:\")\n",
    "            best_model = best_models[metric]\n",
    "            for model_name, p_value in comparisons.items():\n",
    "                if p_value < significance_level:\n",
    "                    print(f\"  {best_model} is significantly better than {model_name} (p = {p_value:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  {best_model} is NOT significantly better than {model_name} (p = {p_value:.4f})\")\n",
    "        except:\n",
    "            print(f\"Rule {metric} not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_t_tests(metrics_df, model_name):\n",
    "    results = {}\n",
    "\n",
    "    for metric in metrics_df.columns:\n",
    "\n",
    "        if model_name == \"SKI MLP\":\n",
    "            continue\n",
    "        \n",
    "        # Retrieve the scores for SKI MLP and Regular MLP\n",
    "        ski_mlp_scores = np.array(metrics_df.loc['SKI MLP', metric][0])\n",
    "        regular_mlp_scores = np.array(metrics_df.loc[model_name, metric][0])\n",
    "        \n",
    "        # Perform a paired t-test between SKI MLP and Regular MLP\n",
    "        t_stat, p_value = stats.ttest_rel(ski_mlp_scores, regular_mlp_scores)\n",
    "        \n",
    "        # Calculate the mean difference\n",
    "        mean_difference = np.mean(ski_mlp_scores - regular_mlp_scores)\n",
    "        \n",
    "        results[metric] = {'p_value': p_value, 'mean_difference': mean_difference}\n",
    "\n",
    "    # Significance level for the tests\n",
    "    significance_level = 0.05\n",
    "\n",
    "    for metric, result in results.items():\n",
    "        try:\n",
    "            p_value = result['p_value']\n",
    "            mean_difference = result['mean_difference']\n",
    "            \n",
    "            print(f\"\\n{metric}:\")\n",
    "            if p_value < significance_level:\n",
    "                if mean_difference > 0:\n",
    "                    print(f\"  SKI MLP is significantly better than {model_name} (p = {p_value:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  {model_name} is significantly better than SKI MLP (p = {p_value:.4f})\")\n",
    "            else:\n",
    "                print(f\"  There is no significant difference between SKI MLP and {model_name} (p = {p_value:.4f})\")\n",
    "        except:\n",
    "            print(f\"Rule {metric} not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, device, max_epochs=epochs):\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for _ in range(max_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            labels = labels.view(-1).long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [type(model).__name__ for model in models]\n",
    "model_names.remove(\"Sequential\")\n",
    "model_names.append(\"Regular MLP\")\n",
    "model_names.append(\"SKI MLP\")\n",
    "metrics = [\"Overall Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "metrics_df = pd.DataFrame([ [ [] for _ in metrics ] for _ in model_names ] , columns=metrics, index=[model_names])\n",
    "\n",
    "seeds = [random.randint(0, 1000) for _ in range(5)]\n",
    "\n",
    "for seed in seeds:\n",
    "    train, val, test = prepare_datasets(data, seed)\n",
    "    for model in models:\n",
    "        model_name = type(model).__name__\n",
    "        if isinstance(model, torch.nn.Module):\n",
    "            kb = KnowledgeBase(\"config.yaml\")\n",
    "            model = kb.predicates[\"PokerHand\"]\n",
    "\n",
    "            # Before Fine Tuning With Logic \n",
    "            train_model(model, kb.loaders[0], kb.device)\n",
    "            metrics_values = evaluate_model(kb.test_loaders[0], model, kb.device)\n",
    "            for metric, value in zip(metrics, metrics_values):\n",
    "                metrics_df.loc[\"Regular MLP\"][metric][0].append(value)\n",
    "            \n",
    "            # After Fine Tuning With Logic \n",
    "            kb.optimize(num_epochs=200, lr=0.000001, early_stopping=True, verbose=False)\n",
    "            model_name = \"SKI MLP\"\n",
    "        else:\n",
    "            model.fit(train.drop(\"Label\", axis=1), train[\"Label\"])\n",
    "\n",
    "        metrics_values = evaluate_model(kb.test_loaders[0], model, kb.device)\n",
    "        for metric, value in zip(metrics, metrics_values):\n",
    "            metrics_df.loc[model_name][metric][0].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics_df, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_names:\n",
    "    perform_t_tests(metrics_df, model)\n",
    "    print()\n",
    "    print( \"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
