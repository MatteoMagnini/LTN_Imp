{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ltn_imp.automation.knowledge_base import KnowledgeBase\n",
    "from ltn_imp.automation.data_loaders import LoaderWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mPoe =>\u001b[0m \u001b[94mmkdir -p examples/datasets\u001b[0m\n",
      "\u001b[37mPoe =>\u001b[0m \u001b[94mcurl -L -o examples/datasets/iris_training.csv https://raw.githubusercontent.com/tommasocarraro/LTNtorch/main/examples/datasets/iris_training.csv\u001b[0m\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2218  100  2218    0     0  10952      0 --:--:-- --:--:-- --:--:-- 10980\n",
      "\u001b[37mPoe =>\u001b[0m \u001b[94mcurl -L -o examples/datasets/iris_test.csv https://raw.githubusercontent.com/tommasocarraro/LTNtorch/main/examples/datasets/iris_test.csv\u001b[0m\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   598  100   598    0     0  12274      0 --:--:-- --:--:-- --:--:-- 12458\n"
     ]
    }
   ],
   "source": [
    "!poetry run poe download-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_samples = 100\n",
    "dataset = torch.rand((nr_samples, 2))\n",
    "labels_dataset = torch.sum(torch.square(dataset - torch.tensor([.5, .5])), dim=1) < .09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"datasets/iris_training.csv\")\n",
    "test_data = pd.read_csv(\"datasets/iris_test.csv\")\n",
    "\n",
    "train_labels = train_data.pop(\"species\")\n",
    "test_labels = test_data.pop(\"species\")\n",
    "\n",
    "train_data = torch.tensor(train_data.to_numpy()).float()\n",
    "test_data = torch.tensor(test_data.to_numpy()).float()\n",
    "train_labels = torch.tensor(train_labels.to_numpy()).long()\n",
    "test_labels = torch.tensor(test_labels.to_numpy()).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryDataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels = self.labels[idxlist[start_idx:end_idx]]\n",
    "            yield data, labels\n",
    "            \n",
    "binary_train_loader = BinaryDataLoader(dataset[:50], labels_dataset[:50], batch_size=64, shuffle=True)\n",
    "binary_test_loader = BinaryDataLoader(dataset[50:], labels_dataset[50:], batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels = self.labels[idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield data, labels\n",
    "\n",
    "multi_train_loader = MultiLabelDataLoader(train_data, train_labels, 64, shuffle=True)\n",
    "multi_test_loader = MultiLabelDataLoader(test_data, test_labels, 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BinaryClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.layer1 = torch.nn.Linear(2, 16)\n",
    "        self.layer2 = torch.nn.Linear(16, 16)\n",
    "        self.layer3 = torch.nn.Linear(16, 2) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.layer1(x))\n",
    "        x = self.elu(self.layer2(x))\n",
    "        logits = self.layer3(x)\n",
    "        return logits\n",
    "\n",
    "# we define predicate P\n",
    "class MultiLabelClassifier(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes=(4, 16, 16, 8, 3)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
    "                                                  for i in range(1, len(layer_sizes))])\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "            if training:\n",
    "                x = self.dropout(x)\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        return logits\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, y, training=True):\n",
    "        logits = self.logits_model(x)\n",
    "        probs = self.softmax(logits)\n",
    "        # y is expected to be a one-hot encoded vector\n",
    "        out = torch.sum(probs * y, dim=1, keepdim=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(loader, model):\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels in loader:\n",
    "        predictions = model.logits_model(data).detach().numpy()\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        mean_accuracy += accuracy_score(labels, predictions)\n",
    "    return mean_accuracy / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = LogitsToPredicate( BinaryClassifier() )  # type: ignore\n",
    "multi = LogitsToPredicate( MultiLabelClassifier() ) # type: ignore\n",
    "\n",
    "predicates = {\"Binary\": binary, \"Multi\": multi}\n",
    "expression_1 = \"( all x. (Binary(x,y)) and (all a. ( Multi(a, b))) )\"\n",
    "\n",
    "rules = [expression_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_loader = LoaderWrapper(variables=[\"x\"], num_classes=2, target=\"y\", loader=binary_train_loader)\n",
    "multi_loader = LoaderWrapper(variables=[\"a\"], num_classes=3, target=\"b\", loader=multi_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_to_data_loader_mapping = {expression_1: [ binary_loader, multi_loader ]} # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBase(expressions=rules, \n",
    "                   predicates=predicates,\n",
    "                   rule_to_data_loader_mapping=rule_to_data_loader_mapping,\n",
    "                   quantifier_impls={\"forall\" : \"pmean_error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(binary_train_loader, binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(binary_test_loader, binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3515625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(multi_train_loader, multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26666666666666666"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(multi_test_loader, multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 0.681427001953125\n",
      "\n",
      "Epoch 101/1000, Loss: 0.485770583152771\n",
      "\n",
      "Epoch 201/1000, Loss: 0.4660830497741699\n",
      "\n",
      "Epoch 301/1000, Loss: 0.44517719745635986\n",
      "\n",
      "Epoch 401/1000, Loss: 0.3358440399169922\n",
      "\n",
      "Epoch 501/1000, Loss: 0.3024158477783203\n",
      "\n",
      "Epoch 601/1000, Loss: 0.28943735361099243\n",
      "\n",
      "Epoch 701/1000, Loss: 0.2582472562789917\n",
      "\n",
      "Epoch 801/1000, Loss: 0.211930513381958\n",
      "\n",
      "Epoch 901/1000, Loss: 0.1780761480331421\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kb.optimize(num_epochs=1000, log_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(binary_train_loader, binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(binary_test_loader, binary) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9821428571428572"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(multi_train_loader, multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(multi_test_loader, multi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
