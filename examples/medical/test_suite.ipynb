{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from ltn_imp.automation.knowledge_base import KnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "state = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"datasets/pima_indians_imputed.csv\", index_col = 0)\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initilize_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating DecisionTree...\n",
      "Best model for DecisionTree saved with recall score of 0.8148\n",
      "\n",
      "Evaluating GradientBoosting...\n",
      "Best model for GradientBoosting saved with recall score of 0.7692\n",
      "\n",
      "Evaluating MultiLayerPerceptron...\n",
      "Best model for MultiLayerPerceptron saved with recall score of 0.7037\n",
      "\n",
      "Evaluating LogisticRegression...\n",
      "Best model for LogisticRegression saved with recall score of 0.8077\n",
      "\n",
      "Evaluating RandomForest...\n",
      "Best model for RandomForest saved with recall score of 0.7037\n",
      "\n",
      "Evaluating KNearestNeighbor...\n",
      "Best model for KNearestNeighbor saved with recall score of 0.7778\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if initilize_models:\n",
    "    # Define the models\n",
    "    models = {\n",
    "        'DecisionTree': DecisionTreeClassifier(random_state=state),\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=state),\n",
    "        'MultiLayerPerceptron': MLPClassifier(random_state=state, max_iter=2000),\n",
    "        'LogisticRegression': LogisticRegression(random_state=state, max_iter=1000),\n",
    "        'RandomForest': RandomForestClassifier(random_state=state),\n",
    "        'KNearestNeighbor': KNeighborsClassifier()\n",
    "    }\n",
    "\n",
    "    # Define parameter grids for each model\n",
    "    param_grids = {\n",
    "        'DecisionTree': {\n",
    "            'max_depth': [None, 5, 10, 15, 20],\n",
    "            'min_samples_split': [5, 10, 20],\n",
    "            'min_samples_leaf': [5, 10]\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': np.linspace(50, 250, 5).astype(int),\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 4, 5]\n",
    "        },\n",
    "        'MultiLayerPerceptron': {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100), (30, 30, 30), (50, 30, 20), (100, 50, 25)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam'],\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "            'learning_rate': ['constant', 'adaptive']\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs']\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': np.linspace(50, 250, 5).astype(int),\n",
    "            #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'max_depth': [None, 5, 10, 15, 20],\n",
    "            'min_samples_split': [5, 10, 20],\n",
    "            'min_samples_leaf': [5, 10]\n",
    "        },\n",
    "        'KNearestNeighbor': {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define the scorer (using recall for refitting as in your example)\n",
    "    scorer = make_scorer(recall_score)\n",
    "\n",
    "    # Perform nested cross-validation\n",
    "    outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    # Iterate over each model to perform grid search and save the best model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        \n",
    "        best_recall = 0\n",
    "        best_model = None\n",
    "        \n",
    "        for train_idx, test_idx in outer_cv.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            # Perform grid search with cross-validation\n",
    "            clf = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=scorer, refit='recall', cv=inner_cv, n_jobs=-1)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            # Check if this model has a better recall score\n",
    "            current_recall = recall_score(y_test, clf.predict(X_test))\n",
    "            if current_recall > best_recall:\n",
    "                best_recall = current_recall\n",
    "                best_model = clf.best_estimator_\n",
    "        \n",
    "        # Save the best model using pickle if it's better than what was previously found\n",
    "        if best_model is not None:\n",
    "            with open(f'models/{model_name}_best_model.pkl', 'wb') as f:\n",
    "                pickle.dump(best_model, f)\n",
    "            print(f\"Best model for {model_name} saved with recall score of {best_recall:.4f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "\n",
    "test_data = pd.read_csv('datasets/pima_indians_imputed.csv', index_col=0).astype(float)\n",
    "\n",
    "y = test_data.iloc[:, -1]\n",
    "\n",
    "x_train, x_test = train_test_split(test_data, test_size=0.5, random_state=seed, stratify=y)\n",
    "\n",
    "x_train.to_csv('datasets/train.csv')\n",
    "x_test.to_csv('datasets/test.csv')\n",
    "\n",
    "x_train, y_train = x_train.iloc[:, :-1], x_train.iloc[:, -1]\n",
    "x_test, y_test = x_test.iloc[:, :-1], x_test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for file_name in os.listdir(\"models\"):\n",
    "    if file_name.endswith('.pkl'):\n",
    "        file_path = os.path.join(\"models\", file_name)\n",
    "        \n",
    "        with open(file_path, 'rb') as file:\n",
    "            trained_model = pickle.load(file)\n",
    "            \n",
    "            # Extract the hyperparameters of the loaded model\n",
    "            model_class = trained_model.__class__\n",
    "            model_params = trained_model.get_params()\n",
    "            \n",
    "            # Reinitialize the model with the same hyperparameters but without the trained state\n",
    "            new_model = model_class(**model_params)\n",
    "            \n",
    "            # Append the reinitialized model to the models list\n",
    "            models.append(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    try:\n",
    "        model.eval()  # Ensure the model is in evaluation mode\n",
    "    except:\n",
    "        \"\"\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        # Ensure x is a tensor and has the right dtype\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        elif x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        try:\n",
    "            probs = model(x)\n",
    "        except:\n",
    "            probs = torch.tensor(model.predict(x))\n",
    "\n",
    "        # Apply binary classification threshold at 0.5\n",
    "        preds = (probs > 0.5).float()\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(model, data_loader):\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, labels in data_loader:\n",
    "            # Ensure data and labels are the correct dtype\n",
    "            if not isinstance(data, torch.Tensor):\n",
    "                data = torch.tensor(data, dtype=torch.float32)\n",
    "            elif data.dtype != torch.float32:\n",
    "                data = data.float()\n",
    "            \n",
    "            if not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            elif labels.dtype != torch.float32:\n",
    "                labels = labels.float()\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = predict(model, data)\n",
    "\n",
    "            # Squeeze predictions and labels to remove dimensions of size 1\n",
    "            predicted_labels = preds.squeeze()\n",
    "            true_labels = labels.squeeze()\n",
    "\n",
    "            # Ensure the shapes match before comparison\n",
    "            if predicted_labels.shape != true_labels.shape:\n",
    "                true_labels = true_labels.view_as(predicted_labels)\n",
    "            \n",
    "            # Collect all predictions and true labels for MCC\n",
    "            all_true_labels.extend(true_labels.cpu().numpy())\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    true_labels = np.array(all_true_labels)\n",
    "    predicted_labels = np.array(all_predicted_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, zero_division=0)  # zero_division=0 handles the division by zero case\n",
    "    recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=0)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "    mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "\n",
    "    try:\n",
    "        model.train()\n",
    "    except:\n",
    "        \"\"\n",
    "\n",
    "    return accuracy, precision, recall, f1, balanced_accuracy, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = KnowledgeBase(\"medical_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "# Iterate over all files in the \"models\" folder\n",
    "\n",
    "for file_name in os.listdir(\"models\"):\n",
    "    if file_name.endswith('.pkl'):\n",
    "        file_path = os.path.join(\"models\", file_name)\n",
    "\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Load the trained model from the pickle file\n",
    "            trained_model = pickle.load(file)\n",
    "            \n",
    "            # Extract the hyperparameters of the loaded model\n",
    "            model_class = trained_model.__class__\n",
    "            model_params = trained_model.get_params()\n",
    "            \n",
    "            # Reinitialize the model with the same hyperparameters but without the trained state\n",
    "            new_model = model_class(**model_params)\n",
    "            \n",
    "            # Append the reinitialized model to the models list\n",
    "            models.append(new_model)\n",
    "            \n",
    "    elif file_name.endswith('.pth'):\n",
    "        file_path = os.path.join(\"models\", file_name)\n",
    "        models.append(torch.load(file_path))\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = kb.loaders[0].loader.dataset.data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n",
    "       'BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "\n",
    "Y = kb.loaders[0].loader.dataset.data['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores= pd.DataFrame([], columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Balanced Accuracy\", \"MCC\"])\n",
    "\n",
    "for model in models:\n",
    "    if hasattr(model,\"fit\"):\n",
    "        model.fit(X, Y)\n",
    "        \n",
    "    accuracy, precision, recall, f1, balanced_accuracy, mcc = compute_metrics(model, kb.test_loaders[0])\n",
    "    scores.loc[model.__class__.__name__] = [accuracy, precision, recall,f1,balanced_accuracy, mcc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sequential</th>\n",
       "      <td>0.763</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>0.719</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.745</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.721</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.612</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.747</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.711</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>0.708</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Accuracy  Precision  Recall  F1 Score  \\\n",
       "Sequential                     0.763      0.619   0.836     0.711   \n",
       "GradientBoostingClassifier     0.719      0.587   0.657     0.620   \n",
       "RandomForestClassifier         0.745      0.634   0.634     0.634   \n",
       "DecisionTreeClassifier         0.750      0.646   0.627     0.636   \n",
       "KNeighborsClassifier           0.721      0.599   0.612     0.605   \n",
       "LogisticRegression             0.747      0.653   0.590     0.620   \n",
       "MLPClassifier                  0.708      0.620   0.425     0.504   \n",
       "\n",
       "                            Balanced Accuracy    MCC  \n",
       "Sequential                              0.780  0.535  \n",
       "GradientBoostingClassifier              0.704  0.399  \n",
       "RandomForestClassifier                  0.719  0.438  \n",
       "DecisionTreeClassifier                  0.721  0.446  \n",
       "KNeighborsClassifier                    0.696  0.390  \n",
       "LogisticRegression                      0.711  0.433  \n",
       "MLPClassifier                           0.643  0.319  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.round(3).sort_values(by=\"Recall\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
