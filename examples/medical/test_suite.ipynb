{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from ltn_imp.automation.knowledge_base import KnowledgeBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "state = 123\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"datasets/pima_indians_imputed.csv\", index_col = 0)\n",
    "X = dataset.iloc[:, :-1]\n",
    "y = dataset.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "initilize_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if initilize_models:\n",
    "    # Define the models\n",
    "    models = {\n",
    "        'DecisionTree': DecisionTreeClassifier(random_state=state),\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=state),\n",
    "        'LogisticRegression': LogisticRegression(random_state=state, max_iter=1000),\n",
    "        'RandomForest': RandomForestClassifier(random_state=state),\n",
    "        'KNearestNeighbor': KNeighborsClassifier()\n",
    "    }\n",
    "\n",
    "    # Define parameter grids for each model\n",
    "    param_grids = {\n",
    "        'DecisionTree': {\n",
    "            'max_depth': [None, 5, 10, 15, 20],\n",
    "            'min_samples_split': [5, 10, 20],\n",
    "            'min_samples_leaf': [5, 10]\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'n_estimators': np.linspace(50, 250, 5).astype(int),\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 4, 5]\n",
    "        },\n",
    "        'LogisticRegression': {\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs']\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'n_estimators': np.linspace(50, 250, 5).astype(int),\n",
    "            #'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'max_depth': [None, 5, 10, 15, 20],\n",
    "            'min_samples_split': [5, 10, 20],\n",
    "            'min_samples_leaf': [5, 10]\n",
    "        },\n",
    "        'KNearestNeighbor': {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    scorer = make_scorer(recall_score)\n",
    "\n",
    "    outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        \n",
    "        best_recall = 0\n",
    "        best_model = None\n",
    "        \n",
    "        for train_idx, test_idx in outer_cv.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            clf = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=scorer, refit='recall', cv=inner_cv, n_jobs=-1)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "            current_recall = recall_score(y_test, clf.predict(X_test))\n",
    "            if current_recall > best_recall:\n",
    "                best_recall = current_recall\n",
    "                best_model = clf.best_estimator_\n",
    "        \n",
    "        # Save the best model using pickle if it's better than what was previously found\n",
    "        if best_model is not None:\n",
    "            with open(f'models/{model_name}_best_model.pkl', 'wb') as f:\n",
    "                pickle.dump(best_model, f)\n",
    "            print(f\"Best model for {model_name} saved with recall score of {best_recall:.4f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(kb):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = kb.predicates[\"Diabetic\"].to(device)\n",
    "    \n",
    "    criteria = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "    patience = 5\n",
    "    min_delta = 0.001\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for _ in range(50):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for data, labels in kb.loaders[0]:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(data)\n",
    "            loss = criteria(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_train_loss = total_loss / num_batches\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        for data, labels in kb.val_loaders[0]:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions = model(data)\n",
    "                val_loss = criteria(predictions, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "                num_val_batches += 1\n",
    "\n",
    "        avg_val_loss = total_val_loss / num_val_batches\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_val_loss + min_delta < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            break\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def predict(model, x):\n",
    "    try:\n",
    "        model.eval()  # Ensure the model is in evaluation mode\n",
    "    except:\n",
    "        \"\"\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        # Ensure x is a tensor and has the right dtype\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        elif x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        try:\n",
    "            x = x.to(device)\n",
    "            probs = model(x)\n",
    "        except:\n",
    "            x = x.to(torch.device(\"cpu\"))\n",
    "            probs = torch.tensor(model.predict(x))\n",
    "\n",
    "        # Apply binary classification threshold at 0.5\n",
    "        preds = (probs > 0.5).float()\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(model, data_loader):\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, labels in data_loader:\n",
    "            # Ensure data and labels are the correct dtype\n",
    "            if not isinstance(data, torch.Tensor):\n",
    "                data = torch.tensor(data, dtype=torch.float32)\n",
    "            elif data.dtype != torch.float32:\n",
    "                data = data.float()\n",
    "            \n",
    "            if not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            elif labels.dtype != torch.float32:\n",
    "                labels = labels.float()\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = predict(model, data)\n",
    "\n",
    "            # Squeeze predictions and labels to remove dimensions of size 1\n",
    "            predicted_labels = preds.squeeze()\n",
    "            true_labels = labels.squeeze()\n",
    "\n",
    "            # Ensure the shapes match before comparison\n",
    "            if predicted_labels.shape != true_labels.shape:\n",
    "                true_labels = true_labels.view_as(predicted_labels)\n",
    "            \n",
    "            # Collect all predictions and true labels for MCC\n",
    "            all_true_labels.extend(true_labels.cpu().numpy())\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    true_labels = np.array(all_true_labels)\n",
    "    predicted_labels = np.array(all_predicted_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, zero_division=0)  # zero_division=0 handles the division by zero case\n",
    "    recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=0)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "    mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "\n",
    "    try:\n",
    "        model.train()\n",
    "    except:\n",
    "        \"\"\n",
    "\n",
    "    return accuracy, precision, recall, f1, balanced_accuracy, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_models = []\n",
    "\n",
    "for file_name in os.listdir(\"models\"):\n",
    "    if file_name.endswith('.pkl'):\n",
    "        file_path = os.path.join(\"models\", file_name)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            trained_model = pickle.load(file)\n",
    "            model_class = trained_model.__class__\n",
    "            model_params = trained_model.get_params()\n",
    "            new_model = model_class(**model_params)\n",
    "            saved_models.append(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "seeds = [random.randint(0, 2000) for _ in range(5)]\n",
    "\n",
    "metrics_df = pd.DataFrame(columns=[\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Balanced Accuracy\", \"MCC\"])\n",
    "test_data = pd.read_csv('datasets/pima_indians_imputed.csv', index_col=0).astype(float)\n",
    "y = test_data.iloc[:, -1]\n",
    "\n",
    "for seed in seeds:\n",
    "    x_train, x_test = train_test_split(test_data, test_size=0.5, random_state=seed, stratify=y)\n",
    "    x_train.to_csv('datasets/train.csv')\n",
    "    x_test.to_csv('datasets/test.csv')\n",
    "    x_train, y_train = x_train.iloc[:, :-1], x_train.iloc[:, -1]\n",
    "    x_test, y_test = x_test.iloc[:, :-1], x_test.iloc[:, -1]\n",
    "\n",
    "    models = saved_models.copy()\n",
    "    kb = KnowledgeBase(\"medical_config.yaml\", device=\"cpu\")\n",
    "    models.append(kb.predicates[\"Diabetic\"])\n",
    "\n",
    "    X = kb.loaders[0].loader.dataset.data[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "    Y = kb.loaders[0].loader.dataset.data['Outcome']\n",
    "\n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "        if model_name == \"Sequential\":\n",
    "            model_name = \"SKI MLP\"\n",
    "\n",
    "        if model_name not in metrics_df.index:\n",
    "            metrics_df.loc[model_name] = [[], [], [], [], [], []] \n",
    "            if model_name == \"SKI MLP\":\n",
    "               metrics_df.loc[\"Regular MLP\"] = [[], [], [], [], [], []] \n",
    "\n",
    "        if hasattr(model, \"fit\"):\n",
    "            model.fit(X, Y)\n",
    "        else:\n",
    "            train(kb)\n",
    "            model = kb.predicates[\"Diabetic\"]\n",
    "            accuracy, precision, recall, f1, balanced_accuracy, mcc = compute_metrics(model, kb.test_loaders[0])\n",
    "            metrics_df.loc[\"Regular MLP\", \"Accuracy\"].append(accuracy)\n",
    "            metrics_df.loc[\"Regular MLP\", \"Precision\"].append(precision)\n",
    "            metrics_df.loc[\"Regular MLP\", \"Recall\"].append(recall)\n",
    "            metrics_df.loc[\"Regular MLP\", \"F1 Score\"].append(f1)\n",
    "            metrics_df.loc[\"Regular MLP\", \"Balanced Accuracy\"].append(balanced_accuracy)\n",
    "            metrics_df.loc[\"Regular MLP\", \"MCC\"].append(mcc)\n",
    "            \n",
    "            kb.optimize(num_epochs=100, log_steps=10, lr=0.000001, early_stopping=True, patience=5, verbose=False)\n",
    "            model = kb.predicates[\"Diabetic\"]\n",
    "        \n",
    "        accuracy, precision, recall, f1, balanced_accuracy, mcc = compute_metrics(model, kb.test_loaders[0])\n",
    "        metrics_df.loc[model_name, \"Accuracy\"].append(accuracy)\n",
    "        metrics_df.loc[model_name, \"Precision\"].append(precision)\n",
    "        metrics_df.loc[model_name, \"Recall\"].append(recall)\n",
    "        metrics_df.loc[model_name, \"F1 Score\"].append(f1)\n",
    "        metrics_df.loc[model_name, \"Balanced Accuracy\"].append(balanced_accuracy)\n",
    "        metrics_df.loc[model_name, \"MCC\"].append(mcc)\n",
    "\n",
    "metrics_to_plot = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Balanced Accuracy\", \"MCC\"]\n",
    "plt.figure(figsize=(15, 25))\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot, 1):\n",
    "    plt.subplot(6, 1, i)\n",
    "    data_to_plot = [metrics_df.loc[model_name, metric] for model_name in metrics_df.index]\n",
    "    \n",
    "    # Customize the boxplots\n",
    "    boxprops = dict(linewidth=2)\n",
    "    medianprops = dict(linewidth=2, color='red')\n",
    "    meanprops = dict(linewidth=2, color='blue')\n",
    "    whiskerprops = dict(linewidth=2)\n",
    "    capprops = dict(linewidth=2)\n",
    "    \n",
    "    plt.boxplot(data_to_plot, labels=metrics_df.index, boxprops=boxprops, \n",
    "                medianprops=medianprops, meanline=True, showmeans=True, \n",
    "                meanprops=meanprops, whiskerprops=whiskerprops, \n",
    "                capprops=capprops)\n",
    "    \n",
    "    plt.title(f'Boxplot of {metric} across different models')\n",
    "    plt.xlabel('Model', fontweight='bold')\n",
    "    plt.ylabel(metric, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "  LogisticRegression is NOT significantly better than GradientBoostingClassifier (p = 0.1822)\n",
      "  LogisticRegression is significantly better than KNeighborsClassifier (p = 0.0020)\n",
      "  LogisticRegression is NOT significantly better than RandomForestClassifier (p = 0.1916)\n",
      "  LogisticRegression is NOT significantly better than DecisionTreeClassifier (p = 0.0971)\n",
      "  LogisticRegression is significantly better than SKI MLP (p = 0.0100)\n",
      "  LogisticRegression is NOT significantly better than Regular MLP (p = 0.0955)\n",
      "\n",
      "Precision:\n",
      "  LogisticRegression is significantly better than GradientBoostingClassifier (p = 0.0415)\n",
      "  LogisticRegression is significantly better than KNeighborsClassifier (p = 0.0008)\n",
      "  LogisticRegression is NOT significantly better than RandomForestClassifier (p = 0.0524)\n",
      "  LogisticRegression is NOT significantly better than DecisionTreeClassifier (p = 0.1705)\n",
      "  LogisticRegression is significantly better than SKI MLP (p = 0.0010)\n",
      "  LogisticRegression is NOT significantly better than Regular MLP (p = 0.1661)\n",
      "\n",
      "Recall:\n",
      "  SKI MLP is significantly better than LogisticRegression (p = 0.0061)\n",
      "  SKI MLP is significantly better than GradientBoostingClassifier (p = 0.0056)\n",
      "  SKI MLP is significantly better than KNeighborsClassifier (p = 0.0014)\n",
      "  SKI MLP is significantly better than RandomForestClassifier (p = 0.0124)\n",
      "  SKI MLP is significantly better than DecisionTreeClassifier (p = 0.0098)\n",
      "  SKI MLP is NOT significantly better than Regular MLP (p = 0.0959)\n",
      "\n",
      "F1 Score:\n",
      "  LogisticRegression is NOT significantly better than GradientBoostingClassifier (p = 0.7408)\n",
      "  LogisticRegression is NOT significantly better than KNeighborsClassifier (p = 0.0557)\n",
      "  LogisticRegression is NOT significantly better than RandomForestClassifier (p = 0.6439)\n",
      "  LogisticRegression is NOT significantly better than DecisionTreeClassifier (p = 0.2546)\n",
      "  LogisticRegression is NOT significantly better than SKI MLP (p = 0.6936)\n",
      "  LogisticRegression is NOT significantly better than Regular MLP (p = 0.1104)\n",
      "\n",
      "Balanced Accuracy:\n",
      "  LogisticRegression is NOT significantly better than GradientBoostingClassifier (p = 0.5784)\n",
      "  LogisticRegression is significantly better than KNeighborsClassifier (p = 0.0276)\n",
      "  LogisticRegression is NOT significantly better than RandomForestClassifier (p = 0.5497)\n",
      "  LogisticRegression is NOT significantly better than DecisionTreeClassifier (p = 0.2553)\n",
      "  LogisticRegression is NOT significantly better than SKI MLP (p = 0.2682)\n",
      "  LogisticRegression is NOT significantly better than Regular MLP (p = 0.0597)\n",
      "\n",
      "MCC:\n",
      "  LogisticRegression is NOT significantly better than GradientBoostingClassifier (p = 0.2934)\n",
      "  LogisticRegression is significantly better than KNeighborsClassifier (p = 0.0064)\n",
      "  LogisticRegression is NOT significantly better than RandomForestClassifier (p = 0.3036)\n",
      "  LogisticRegression is NOT significantly better than DecisionTreeClassifier (p = 0.1665)\n",
      "  LogisticRegression is NOT significantly better than SKI MLP (p = 0.0668)\n",
      "  LogisticRegression is NOT significantly better than Regular MLP (p = 0.0580)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "best_models = {}\n",
    "for metric in metrics_df.columns:\n",
    "    mean_scores = metrics_df[metric].apply(np.mean)\n",
    "    best_model = mean_scores.idxmax()  \n",
    "    best_models[metric] = best_model\n",
    "\n",
    "results = {}\n",
    "for metric in metrics_df.columns:\n",
    "    best_model = best_models[metric]\n",
    "    best_scores = metrics_df.loc[best_model, metric]\n",
    "    \n",
    "    results[metric] = {}\n",
    "    \n",
    "    for model_name in metrics_df.index:\n",
    "        if model_name == best_model:\n",
    "            continue\n",
    "        \n",
    "        comparison_scores = metrics_df.loc[model_name, metric]\n",
    "        t_stat, p_value = stats.ttest_rel(best_scores, comparison_scores)\n",
    "        results[metric][model_name] = p_value \n",
    "\n",
    "significance_level = 0.05\n",
    "for metric, comparisons in results.items():\n",
    "    print(f\"\\n{metric}:\")\n",
    "    best_model = best_models[metric]\n",
    "    for model_name, p_value in comparisons.items():\n",
    "        if p_value < significance_level:\n",
    "            print(f\"  {best_model} is significantly better than {model_name} (p = {p_value:.4f})\")\n",
    "        else:\n",
    "            print(f\"  {best_model} is NOT significantly better than {model_name} (p = {p_value:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
