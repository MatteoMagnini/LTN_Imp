{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import copy \n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from ltn_imp.automation.knowledge_base import KnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clump_thickness</th>\n",
       "      <th>Uniformity_of_cell_size</th>\n",
       "      <th>Uniformity_of_cell_shape</th>\n",
       "      <th>Marginal_adhesion</th>\n",
       "      <th>Single_epithelial_cell_size</th>\n",
       "      <th>Bare_nuclei</th>\n",
       "      <th>Bland_chromatin</th>\n",
       "      <th>Normal_nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>50</td>\n",
       "      <td>70</td>\n",
       "      <td>100.0</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clump_thickness  Uniformity_of_cell_size  Uniformity_of_cell_shape  \\\n",
       "0               50                       10                        10   \n",
       "1               50                       40                        40   \n",
       "2               30                       10                        10   \n",
       "3               60                       80                        80   \n",
       "4               40                       10                        10   \n",
       "\n",
       "   Marginal_adhesion  Single_epithelial_cell_size  Bare_nuclei  \\\n",
       "0                 10                           20         10.0   \n",
       "1                 50                           70        100.0   \n",
       "2                 10                           20         20.0   \n",
       "3                 10                           30         40.0   \n",
       "4                 30                           20         10.0   \n",
       "\n",
       "   Bland_chromatin  Normal_nucleoli  Mitoses  Label  \n",
       "0               30               10       10      0  \n",
       "1               30               20       10      0  \n",
       "2               30               10       10      0  \n",
       "3               30               70       10      0  \n",
       "4               30               10       10      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer_wisconsin_original = fetch_ucirepo(id=15) \n",
    "X = breast_cancer_wisconsin_original.data.features \n",
    "y = breast_cancer_wisconsin_original.data.targets \n",
    "data = pd.DataFrame(X, columns=breast_cancer_wisconsin_original.data.feature_names)\n",
    "data = data * 10\n",
    "\n",
    "data[\"Label\"] = y\n",
    "data[\"Label\"] = data[\"Label\"].map({4: 1, 2: 0})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Clump_thickness', 'Uniformity_of_cell_size',\n",
       "       'Uniformity_of_cell_shape', 'Marginal_adhesion',\n",
       "       'Single_epithelial_cell_size', 'Bare_nuclei', 'Bland_chromatin',\n",
       "       'Normal_nucleoli', 'Mitoses', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clump_thickness                 0\n",
      "Uniformity_of_cell_size         0\n",
      "Uniformity_of_cell_shape        0\n",
      "Marginal_adhesion               0\n",
      "Single_epithelial_cell_size     0\n",
      "Bare_nuclei                    16\n",
      "Bland_chromatin                 0\n",
      "Normal_nucleoli                 0\n",
      "Mitoses                         0\n",
      "Label                           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isna().sum())\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(data, random_seed=42):    \n",
    "    X = data.drop(\"Label\", axis=1)  # Features\n",
    "    y = data[\"Label\"]  # Labels\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed, stratify=y)    \n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    test_data = pd.concat([X_test, y_test], axis=1)\n",
    "    train_data.to_csv(\"datasets/train.csv\", index=False)\n",
    "    test_data.to_csv(\"datasets/test.csv\", index=False)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478, 10)\n",
      "(205, 10)\n"
     ]
    }
   ],
   "source": [
    "prepare_datasets(data)\n",
    "train = pd.read_csv(\"datasets/train.csv\")\n",
    "test = pd.read_csv(\"datasets/test.csv\")\n",
    "data = pd.concat([train, test])\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "0    444\n",
       "1    239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts(\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def find_best_models(X,y):\n",
    "    param_grid = {\n",
    "\n",
    "        'KNN': {\n",
    "            'model': KNeighborsClassifier(),\n",
    "            'params': {\n",
    "                'n_neighbors': [3, 5, 7],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'p': [1, 2]\n",
    "            }\n",
    "        },\n",
    "\n",
    "        'DT': {\n",
    "            'model': DecisionTreeClassifier(),\n",
    "            'params': {\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'criterion': ['gini', 'entropy']\n",
    "            }\n",
    "        },\n",
    "\n",
    "        'RF': {\n",
    "            'model': RandomForestClassifier(),\n",
    "            'params': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'criterion': ['gini', 'entropy']\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        'LR': {\n",
    "            'model': LogisticRegression(max_iter=1000),\n",
    "            'params': {\n",
    "                'C': [0.1, 1, 10],\n",
    "                'solver': ['lbfgs', 'liblinear']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    for name, model_info in param_grid.items():\n",
    "        grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=5, n_jobs=-1)\n",
    "        grid_search.fit(X, y)\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "\n",
    "    models = []\n",
    "    for name, model in best_models.items():\n",
    "        if name == 'KNN':\n",
    "            models.append(KNeighborsClassifier(**model.get_params()))\n",
    "        elif name == 'DT':\n",
    "            models.append(DecisionTreeClassifier(**model.get_params()))\n",
    "        elif name == 'RF':\n",
    "            models.append(RandomForestClassifier(**model.get_params()))\n",
    "        elif name == 'LR':\n",
    "            models.append(LogisticRegression(**model.get_params()))\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, dropout_prob=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        \n",
    "        # Build layers with dropout\n",
    "        for out_size in hidden_layers:\n",
    "            layers.append(nn.Linear(in_size, out_size))\n",
    "            layers.append(nn.LeakyReLU())  # LeakyReLU activation\n",
    "            layers.append(nn.Dropout(p=dropout_prob))  # Dropout layer\n",
    "            in_size = out_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(in_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.07s/trial, best loss: 0.5463007092475891]\n",
      "Best hyperparameters: {'dropout_prob': np.float64(0.19497173581503247), 'hidden_layer_sizes': np.int64(3), 'learning_rate': np.float64(0.10621130558425448), 'num_epochs': np.float64(17.0), 'weight_decay': np.float64(0.03270094762285982)}\n"
     ]
    }
   ],
   "source": [
    "kb = KnowledgeBase(\"config.yaml\")\n",
    "\n",
    "def objective(params):\n",
    "    learning_rate = params['learning_rate']\n",
    "    weight_decay = params['weight_decay']\n",
    "    hidden_layer_sizes = params['hidden_layer_sizes']\n",
    "    num_epochs = params['num_epochs']\n",
    "    dropout_prob = params['dropout_prob'] \n",
    "\n",
    "    model = MLP(input_size=9, output_size=1, hidden_layers=hidden_layer_sizes, dropout_prob=dropout_prob)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs): \n",
    "        for batch_X, batch_y in kb.loaders[0]:  \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in kb.test_loaders[0]:  \n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -6, -1),  # 1e-5 to 1\n",
    "    'weight_decay': hp.loguniform('weight_decay', -5, -1),   # 1e-6 to 1e-2\n",
    "\n",
    "    'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [\n",
    "        [512, 256, 128, 64, 32],  # Architecture with 5 layers\n",
    "        [256, 128, 64, 32],       # Architecture with 4 layers\n",
    "        [512, 256, 128],          # Architecture with 3 layers\n",
    "        [256, 128],               # Architecture with 2 layers\n",
    "        [512],                    # Architecture with 1 layer\n",
    "    ]),\n",
    "\n",
    "    'num_epochs': scope.int(hp.quniform('num_epochs', 5, 50, 1)),  # Epochs range from 5 to 200\n",
    "    'dropout_prob': hp.uniform('dropout_prob', 0.1, 0.5)  # Dropout probability between 0.1 and 0.5\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,  # Use the TPE algorithm\n",
    "    max_evals=1,  \n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001832278642898956\n",
    "weight_decay = 0.15047216160202792\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = find_best_models(train.drop(\"Label\", axis=1), train[\"Label\"])\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=1000, random_state=42)\n",
    "models.append(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append(kb.predicates[\"Cancer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KNeighborsClassifier',\n",
       " 'DecisionTreeClassifier',\n",
       " 'RandomForestClassifier',\n",
       " 'LogisticRegression',\n",
       " 'MLPClassifier',\n",
       " 'LogicTensorNetwork']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [type(model).__name__ for model in models]\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Balanced Accuracy\", \"MCC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This section includes all methods needed to train and evalute all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def predict(model, x):\n",
    "    try:\n",
    "        model.eval()  # Ensure the model is in evaluation mode\n",
    "    except:\n",
    "        \"\"\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        # Ensure x is a tensor and has the right dtype\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        elif x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "\n",
    "        try:\n",
    "            probs = model(x)\n",
    "        except Exception as e:\n",
    "            x = x.to(torch.device(\"cpu\"))\n",
    "            probs = torch.tensor(model.predict(x))\n",
    "\n",
    "        # Apply binary classification threshold at 0.5\n",
    "        preds = (probs > 0.5).float()\n",
    "    return preds\n",
    "\n",
    "def compute_metrics(model, data_loader):\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, labels in data_loader:\n",
    "            # Ensure data and labels are the correct dtype\n",
    "            if not isinstance(data, torch.Tensor):\n",
    "                data = torch.tensor(data, dtype=torch.float32)\n",
    "            elif data.dtype != torch.float32:\n",
    "                data = data.float()\n",
    "            \n",
    "            if not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            elif labels.dtype != torch.float32:\n",
    "                labels = labels.float()\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = predict(model, data)\n",
    "\n",
    "            # Squeeze predictions and labels to remove dimensions of size 1\n",
    "            predicted_labels = preds.squeeze()\n",
    "            true_labels = labels.squeeze()\n",
    "\n",
    "            # Ensure the shapes match before comparison\n",
    "            if predicted_labels.shape != true_labels.shape:\n",
    "                true_labels = true_labels.view_as(predicted_labels)\n",
    "            \n",
    "            # Collect all predictions and true labels for MCC\n",
    "            all_true_labels.extend(true_labels.cpu().numpy())\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    true_labels = np.array(all_true_labels)\n",
    "    predicted_labels = np.array(all_predicted_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, zero_division=0)  # zero_division=0 handles the division by zero case\n",
    "    recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=0)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "    mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "\n",
    "    try:\n",
    "        model.train()\n",
    "    except:\n",
    "        \"\"\n",
    "\n",
    "    return accuracy, precision, recall, f1, balanced_accuracy, mcc\n",
    "\n",
    "# Rule 1: Malignant if Bare_nuclei > 55\n",
    "def rule_1(data_df):\n",
    "    condition = (data_df['Bare_nuclei'] > 55)\n",
    "    outcome = pd.Series(np.nan, index=data_df.index)\n",
    "    outcome[condition] = 1  # Malignant\n",
    "    return outcome\n",
    "\n",
    "# Rule 2: Malignant if Clump_thickness > 65\n",
    "def rule_2(data_df):\n",
    "    condition = (data_df['Clump_thickness'] > 65)\n",
    "    outcome = pd.Series(np.nan, index=data_df.index)\n",
    "    outcome[condition] = 1  # Malignant\n",
    "    return outcome\n",
    "\n",
    "# Rule 3: Benign if Bare_nuclei <= 55 and Clump_thickness <= 65\n",
    "def rule_3(data_df):\n",
    "    condition = (data_df['Bare_nuclei'] <= 55) & (data_df['Clump_thickness'] <= 65)\n",
    "    outcome = pd.Series(np.nan, index=data_df.index)\n",
    "    outcome[condition] = 0  # Benign\n",
    "    return outcome\n",
    "\n",
    "# Rule 4: Benign if Bare_nuclei <= 15\n",
    "def rule_4(data_df):\n",
    "    condition = (data_df['Bare_nuclei'] <= 15)\n",
    "    outcome = pd.Series(np.nan, index=data_df.index)\n",
    "    outcome[condition] = 0  # Benign\n",
    "    return outcome\n",
    "\n",
    "# Rule 5: Malignant if Bare_nuclei > 15 and Clump_thickness > 45\n",
    "def rule_5(data_df):\n",
    "    condition = (data_df['Bare_nuclei'] > 15) & (data_df['Clump_thickness'] > 45)\n",
    "    outcome = pd.Series(np.nan, index=data_df.index)\n",
    "    outcome[condition] = 1  # Malignant\n",
    "    return outcome\n",
    "\n",
    "# Rule 6: Malignant if Bare_nuclei > 65 and Clump_thickness <= 45\n",
    "def rule_6(data_df):\n",
    "    condition = (data_df['Bare_nuclei'] > 65) & (data_df['Clump_thickness'] <= 45)\n",
    "    outcome = pd.Series(np.nan, index=data_df.index)\n",
    "    outcome[condition] = 1  # Malignant\n",
    "    return outcome\n",
    "\n",
    "# Rule 7: Benign if 15 < Bare_nuclei <= 65 and Clump_thickness <= 45\n",
    "def rule_7(data_df):\n",
    "    condition = (data_df['Bare_nuclei'] > 15) & (data_df['Bare_nuclei'] <= 65) & (data_df['Clump_thickness'] <= 45)\n",
    "    outcome = pd.Series(np.nan, index=data_df.index)\n",
    "    outcome[condition] = 0  # Benign\n",
    "    return outcome\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def rule_adherence(model, dataset):\n",
    "\n",
    "    dataset = TensorDataset(torch.tensor(dataset.drop(\"Label\", axis=1).values, dtype=torch.float32), \n",
    "                            torch.tensor(dataset[\"Label\"].values, dtype=torch.float32))\n",
    "    \n",
    "    data_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    adherence_scores = {f'Rule {i}': 0 for i in range(1, len(kb.rules) - 1)}\n",
    "    total_relevant = {f'Rule {i}': 0 for i in range(1, len(kb.rules) - 1)}\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            if isinstance(data, torch.Tensor):\n",
    "                data = data.numpy()\n",
    "\n",
    "            data_df = pd.DataFrame(data, columns=columns)\n",
    "            preds = predict(model, data).squeeze().numpy()\n",
    "\n",
    "            for i in range(1, len(kb.rules) - 1):\n",
    "                rule_func = globals()[f'rule_{i}']\n",
    "                expected_outcome = rule_func(data_df)\n",
    "\n",
    "                relevant_indices = expected_outcome.notna()\n",
    "                relevant_preds = preds[relevant_indices]\n",
    "                relevant_expected = expected_outcome[relevant_indices]\n",
    "\n",
    "                adherence = (relevant_expected == relevant_preds).astype(int)\n",
    "                \n",
    "                adherence_scores[f'Rule {i}'] += adherence.sum()\n",
    "                total_relevant[f'Rule {i}'] += len(adherence)\n",
    "\n",
    "    adherence_percentages = {rule: (adherence_scores[rule] / total_relevant[rule]) * 100 \n",
    "                             for rule in adherence_scores}\n",
    "    \n",
    "    return adherence_percentages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader,  device):\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(loader, model, device):\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, labels in loader:\n",
    "            # Ensure data and labels are the correct dtype\n",
    "            if not isinstance(data, torch.Tensor):\n",
    "                data = torch.tensor(data, dtype=torch.float32)\n",
    "            elif data.dtype != torch.float32:\n",
    "                data = data.float()\n",
    "            \n",
    "            if not isinstance(labels, torch.Tensor):\n",
    "                labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            elif labels.dtype != torch.float32:\n",
    "                labels = labels.float()\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = predict(model, data)\n",
    "\n",
    "            # Squeeze predictions and labels to remove dimensions of size 1\n",
    "            predicted_labels = preds.squeeze()\n",
    "            true_labels = labels.squeeze()\n",
    "\n",
    "            # Ensure the shapes match before comparison\n",
    "            if predicted_labels.shape != true_labels.shape:\n",
    "                true_labels = true_labels.view_as(predicted_labels)\n",
    "            \n",
    "            # Collect all predictions and true labels for MCC\n",
    "            all_true_labels.extend(true_labels.cpu().numpy())\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    true_labels = np.array(all_true_labels)\n",
    "    predicted_labels = np.array(all_predicted_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, zero_division=0)  # zero_division=0 handles the division by zero case\n",
    "    recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=0)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "    mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "\n",
    "    try:\n",
    "        model.train()\n",
    "    except:\n",
    "        \"\"\n",
    "\n",
    "    return accuracy, precision, recall, f1, balanced_accuracy, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn_model(dataset, model):\n",
    "    # Assuming the last column is the label\n",
    "    data = dataset.iloc[:, :-1]  # All columns except the last one are features\n",
    "    true_labels = dataset.iloc[:, -1]  # The last column is the label\n",
    "\n",
    "    # Predict using the sklearn model\n",
    "    predicted_labels = model.predict(data)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, zero_division=0)  # zero_division=0 handles the division by zero case\n",
    "    recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predicted_labels, zero_division=0)\n",
    "    balanced_accuracy = balanced_accuracy_score(true_labels, predicted_labels)\n",
    "    mcc = matthews_corrcoef(true_labels, predicted_labels)\n",
    "    return accuracy, precision, recall, f1, balanced_accuracy, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "kb = KnowledgeBase(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import copy \n",
    "\n",
    "metrics_df = pd.DataFrame([ [ [] for _ in metrics ] for _ in model_names ] , columns=metrics, index=[model_names])\n",
    "adherence_df = pd.DataFrame([ [ [] for _ in range(1, len(kb.rules) - 1) ] for _ in model_names ] , columns=[f\"Rule {i}\" for i in range(1, len(kb.rules) - 1)], index=[model_names])\n",
    "\n",
    "seeds = [seed for seed in range(0, 20)]\n",
    "\n",
    "for seed in seeds:\n",
    "    train, test = prepare_datasets(data, seed)\n",
    "    for model in models:\n",
    "        model_name = type(model).__name__\n",
    "        if not hasattr(model, \"fit\"):\n",
    "            kb = KnowledgeBase(\"config.yaml\")            \n",
    "            kb.optimize(num_epochs=epochs, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "            model = copy.deepcopy( kb.predicates[\"Cancer\"] ) \n",
    "            metrics_values = evaluate_model(kb.test_loaders[0], model, kb.device)\n",
    "            adherence_values = rule_adherence(model, test)\n",
    "\n",
    "            for metric, value in zip(metrics, metrics_values):\n",
    "                metrics_df.loc[model_name][metric][0].append(value)\n",
    "\n",
    "            for rule, adherence in adherence_values.items():\n",
    "                adherence_df.loc[model_name][rule][0].append(adherence)\n",
    "            \n",
    "        else:\n",
    "            model.fit(train.drop(\"Label\", axis=1), train[\"Label\"])\n",
    "            metrics_values = evaluate_sklearn_model(test, model)\n",
    "            adherence_values = rule_adherence(model, test)\n",
    "\n",
    "            for metric, value in zip(metrics, metrics_values):\n",
    "                metrics_df.loc[model_name][metric][0].append(value)\n",
    "\n",
    "            for rule, adherence in adherence_values.items():\n",
    "                adherence_df.loc[model_name][rule][0].append(adherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adherence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with 50 Random States\n",
    "\n",
    "This section evaluates all models across 50 different random states in performance metrics. The following steps are performed:\n",
    "\n",
    " ### 1. Box Plot Visualization:  \n",
    "   Box plots are generated for the evaluation metrics, allowing a clear visual comparison of model performance.\n",
    "   \n",
    "   \n",
    "\n",
    " ### 2. T-Test Comparisons:  \n",
    "   Two sets of T-tests are conducted:\n",
    "   \n",
    "   - **Best Model Significance Test**:  \n",
    "     For each evaluation metric, the T-test checks if the best-performing model is significantly better than the others.\n",
    "   \n",
    "   - **Comparison with Our Model**:  \n",
    "     For cases where our model is not the best, it is compared against each of the other models for each metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_df, metrics_to_plot):\n",
    "    plt.figure(figsize=(18, len(metrics_to_plot) * 4))\n",
    "\n",
    "    for i, metric in enumerate(metrics_to_plot, 1):\n",
    "        plt.subplot(len(metrics_to_plot), 1, i)\n",
    "        data_to_plot = [metrics_df.loc[model_name, metric] for model_name in metrics_df.index]\n",
    "        \n",
    "        # Customize the boxplots\n",
    "        boxprops = dict(linewidth=2)\n",
    "        medianprops = dict(linewidth=2, color='red')\n",
    "        meanprops = dict(linewidth=2, color='blue')\n",
    "        whiskerprops = dict(linewidth=2)\n",
    "        capprops = dict(linewidth=2)\n",
    "        \n",
    "        plt.boxplot(data_to_plot, labels=metrics_df.index, boxprops=boxprops, \n",
    "                    medianprops=medianprops, meanline=True, showmeans=True, \n",
    "                    meanprops=meanprops, whiskerprops=whiskerprops, \n",
    "                    capprops=capprops)\n",
    "        \n",
    "        plt.title(f'Boxplot of {metric} across different models')\n",
    "        plt.xlabel('Model', fontweight='bold')\n",
    "        plt.ylabel(metric, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def compare_models(metrics_df):\n",
    "\n",
    "    best_models = {}\n",
    "    for metric in metrics_df.columns:\n",
    "        try:\n",
    "            mean_scores = metrics_df[metric].apply(np.mean)\n",
    "            best_model = mean_scores.idxmax()  \n",
    "            best_models[metric] = best_model\n",
    "        except:\n",
    "            print(f\"Rule {metric} not available\")\n",
    "            continue\n",
    "\n",
    "    results = {}\n",
    "    for metric in metrics_df.columns:\n",
    "        try:   \n",
    "            best_model = best_models[metric]\n",
    "            best_scores = metrics_df.loc[best_model, metric]\n",
    "            \n",
    "            results[metric] = {}\n",
    "            \n",
    "            for model_name in metrics_df.index:\n",
    "                if model_name == best_model:\n",
    "                    continue\n",
    "                \n",
    "                comparison_scores = metrics_df.loc[model_name, metric]\n",
    "                t_stat, p_value = stats.ttest_rel(best_scores, comparison_scores)\n",
    "                results[metric][model_name] = p_value \n",
    "\n",
    "        except:\n",
    "            print(f\"Rule {metric} not available\")\n",
    "            continue\n",
    "\n",
    "    significance_level = 0.05\n",
    "    for metric, comparisons in results.items():\n",
    "        try:\n",
    "            print(f\"\\n{metric}:\")\n",
    "            best_model = best_models[metric]\n",
    "            for model_name, p_value in comparisons.items():\n",
    "                if p_value < significance_level:\n",
    "                    print(f\"  {best_model} is significantly better than {model_name} (p = {p_value:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  {best_model} is NOT significantly better than {model_name} (p = {p_value:.4f})\")\n",
    "        except:\n",
    "            print(f\"Rule {metric} not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_t_tests(metrics_df, model_name):\n",
    "    results = {}\n",
    "\n",
    "    for metric in metrics_df.columns:\n",
    "\n",
    "        if model_name == \"LogicTensorNetwork\":\n",
    "            continue\n",
    "        \n",
    "        ski_mlp_scores = np.array(metrics_df.loc['LogicTensorNetwork', metric])\n",
    "        regular_mlp_scores = np.array(metrics_df.loc[model_name, metric])\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_rel(ski_mlp_scores, regular_mlp_scores)\n",
    "        \n",
    "        # Calculate the mean difference\n",
    "        mean_difference = np.mean(ski_mlp_scores - regular_mlp_scores)\n",
    "        \n",
    "        results[metric] = {'p_value': p_value, 'mean_difference': mean_difference}\n",
    "\n",
    "    # Significance level for the tests\n",
    "    significance_level = 0.05\n",
    "\n",
    "    for metric, result in results.items():\n",
    "\n",
    "        if model_name == \"LogicTensorNetwork\":\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            p_value = result['p_value']\n",
    "            mean_difference = result['mean_difference']\n",
    "            \n",
    "            print(f\"\\n{metric}:\")\n",
    "            if p_value < significance_level:\n",
    "                if mean_difference > 0:\n",
    "                    print(f\"  LogicTensorNetwork is significantly better than {model_name} (p = {p_value:.4f})\")\n",
    "                else:\n",
    "                    print(f\"  {model_name} is significantly better than LogicTensorNetwork (p = {p_value:.4f})\")\n",
    "            else:\n",
    "                print(f\"  There is no significant difference between LogicTensorNetwork and {model_name} (p = {p_value:.4f})\")\n",
    "        except:\n",
    "            print(f\"Rule {metric} not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.index = model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics_df, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in metrics_df.index:\n",
    "    perform_t_tests(metrics_df, model)\n",
    "    print()\n",
    "    print( \"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic Adherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation with 50 Random States\n",
    "\n",
    "This section evaluates all models across 50 different random states in Logic adherence ( meaning how well aligned are the predictions with the given logic rules ). The following steps are performed:\n",
    "\n",
    " ### 1. Box Plot Visualization:  \n",
    "   Box plots are generated for the evaluation metrics, allowing a clear visual comparison of model performance.\n",
    "   \n",
    "   \n",
    "\n",
    " ### 2. T-Test Comparisons:  \n",
    "   Two sets of T-tests are conducted:\n",
    "   \n",
    "   - **Best Model Significance Test**:  \n",
    "     For each rule, the T-test checks if the best-performing model is significantly better than the others.\n",
    "   \n",
    "   - **Comparison with Our Model**:  \n",
    "     For cases where our model is not the best, it is compared against each of the other models for each rule.\n",
    "\n",
    "### 3. Rules:\n",
    "\n",
    "  Rule 1: forall person. (((person[BMI] > 29) and (person[Glucose] > 125 )) -> Diabetic(person))\"\n",
    "  \n",
    "  Rule 2: forall person. (((person[BMI] < 26) and (person[Glucose] < 101 )) -> not Diabetic(person))\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "adherence_df.index = model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics( adherence_df, [\"Rule \" + str(i) for i in range(1, 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models(adherence_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in metrics_df.index:\n",
    "    perform_t_tests(adherence_df, model)\n",
    "    print()\n",
    "    print( \"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
