{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_samples = 100\n",
    "dataset = torch.rand((nr_samples, 2))\n",
    "labels_dataset = torch.sum(torch.square(dataset - torch.tensor([.5, .5])), dim=1) < .09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels = self.labels[idxlist[start_idx:end_idx]]\n",
    "            yield data, labels\n",
    "            \n",
    "train_loader = DataLoader(dataset[:50], labels_dataset[:50], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset[50:], labels_dataset[50:], batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class ModelA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "        self.elu = nn.ELU()\n",
    "        self.layer1 = nn.Linear(2, 16)\n",
    "        self.layer2 = nn.Linear(16, 16)\n",
    "        self.layer3 = nn.Linear(16, 1)  # Output a single value per input\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.layer1(x))\n",
    "        x = self.elu(self.layer2(x))\n",
    "        logits = self.layer3(x)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs\n",
    "\n",
    "    def predict(self, x):\n",
    "        probs = self.forward(x)\n",
    "        preds = (probs > 0.49999).float()\n",
    "        return preds\n",
    "\n",
    "    def compute_accuracy(self, data_loader):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, labels in data_loader:\n",
    "                preds = self.predict(data)\n",
    "                predicted_labels = preds.squeeze()\n",
    "                true_labels = labels.float()\n",
    "                correct += (predicted_labels == true_labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ModelA() \n",
    "a.compute_accuracy(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.compute_accuracy(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all x.((y = pos) -> Classifier(x))', 'all x.((y = neg) -> -Classifier(x))']\n",
      "Results: [tensor(0.3659, grad_fn=<AminBackward0>), tensor(0.6061, grad_fn=<AminBackward0>)]\n",
      "Loss: tensor(0.5278, grad_fn=<RsubBackward1>)\n",
      "\n",
      "['all x.((y = pos) -> Classifier(x))', 'all x.((y = neg) -> -Classifier(x))']\n",
      "Results: [tensor(0.5002, grad_fn=<AminBackward0>), tensor(0.4998, grad_fn=<AminBackward0>)]\n",
      "Loss: tensor(0.5000, grad_fn=<RsubBackward1>)\n",
      "\n",
      "['all x.((y = pos) -> Classifier(x))', 'all x.((y = neg) -> -Classifier(x))']\n",
      "Results: [tensor(0.5005, grad_fn=<AminBackward0>), tensor(0.4995, grad_fn=<AminBackward0>)]\n",
      "Loss: tensor(0.5000, grad_fn=<RsubBackward1>)\n",
      "\n",
      "['all x.((y = pos) -> Classifier(x))', 'all x.((y = neg) -> -Classifier(x))']\n",
      "Results: [tensor(0.5001, grad_fn=<AminBackward0>), tensor(0.4999, grad_fn=<AminBackward0>)]\n",
      "Loss: tensor(0.5000, grad_fn=<RsubBackward1>)\n",
      "\n",
      "['all x.((y = pos) -> Classifier(x))', 'all x.((y = neg) -> -Classifier(x))']\n",
      "Results: [tensor(0.5007, grad_fn=<AminBackward0>), tensor(0.4992, grad_fn=<AminBackward0>)]\n",
      "Loss: tensor(0.5001, grad_fn=<RsubBackward1>)\n",
      "\n",
      "['all x.((y = pos) -> Classifier(x))', 'all x.((y = neg) -> -Classifier(x))']\n",
      "Results: [tensor(0.5002, grad_fn=<AminBackward0>), tensor(0.5000, grad_fn=<AminBackward0>)]\n",
      "Loss: tensor(0.4999, grad_fn=<RsubBackward1>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ltn_imp.fuzzy_operators.aggregators import SatAgg\n",
    "from ltn_imp.parsing.parser import LTNConverter\n",
    "from nltk.sem.logic import Expression\n",
    "\n",
    "# Define the rules\n",
    "expression_1 = \"all x. ((y = pos) -> Classifier(x))\"\n",
    "expression_2 = \"all x. ((y = neg) -> not Classifier(x))\"\n",
    "\n",
    "rules = [expression_1, expression_2]\n",
    "\n",
    "# Initialize the satisfaction aggregator and optimizer\n",
    "sat_agg = SatAgg()\n",
    "optimizer = torch.optim.Adam(list(a.parameters()), lr=0.001)\n",
    "converter = LTNConverter(predicates={\"Classifier\": a}, quantifier_impls={\"forall\": \"min\"})\n",
    "\n",
    "# Convert the rules\n",
    "rules = [converter(rule, process=False) for rule in rules]\n",
    "\n",
    "log_steps = 400\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2001):\n",
    "    results = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        data, label = batch\n",
    "\n",
    "        # Convert labels to one-hot encoding\n",
    "        label = torch.stack([torch.tensor([1., 0.]) if l == 1 else torch.tensor([0., 1.]) for l in label])\n",
    "\n",
    "        # Create variable mapping for the entire batch\n",
    "        var_mapping = {\"x\": data, \"y\": label, \"pos\": torch.tensor([1., 0.]), \"neg\": torch.tensor([0., 1.])}\n",
    "\n",
    "        # Apply rules to the entire batch\n",
    "        for rule in rules:\n",
    "            results.append(rule(var_mapping))\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = 1- sat_agg(*results)\n",
    "\n",
    "    # Ensure loss is a tensor and connected to the computation graph\n",
    "    assert isinstance(loss, torch.Tensor), \"Loss is not a tensor\"\n",
    "    assert loss.grad_fn is not None, \"Loss has no grad_fn, indicating it is not connected to the computation graph\"\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Logging\n",
    "    if epoch % log_steps == 0:\n",
    "        print([str(rule) for rule in rules])\n",
    "        print(\"Results:\", results)\n",
    "        print(\"Loss:\", loss)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.compute_accuracy(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.compute_accuracy(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
