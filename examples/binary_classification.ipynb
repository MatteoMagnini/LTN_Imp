{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ltn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from ltn_imp.parsing import convert_to_ltn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_samples = 100\n",
    "dataset = torch.rand((nr_samples, 2))\n",
    "labels_dataset = torch.sum(torch.square(dataset - torch.tensor([.5, .5])), dim=1) < .09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelA, self).__init__()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.layer1 = torch.nn.Linear(2, 16)\n",
    "        self.layer2 = torch.nn.Linear(16, 16)\n",
    "        self.layer3 = torch.nn.Linear(16, 1)\n",
    "        self.elu = torch.nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.elu(self.layer1(x))\n",
    "        x = self.elu(self.layer2(x))\n",
    "        return self.sigmoid(self.layer3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels = self.labels[idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield data, labels\n",
    "\n",
    "train_loader = DataLoader(dataset[:50], labels_dataset[:50], 64, True)\n",
    "test_loader = DataLoader(dataset[50:], labels_dataset[50:], 64, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelA()\n",
    "predicates = {\"Classifier\": model }\n",
    "\n",
    "expression_1 = \"all x. (Classifier(x))\"\n",
    "rule_1 = convert_to_ltn(expression_1, predicates=predicates, functions=None, quantifier_impls={\"forall\" : \"pmean_error\"})\n",
    "\n",
    "expression_2 = \" all x. not(Classifier(x))\"\n",
    "rule_2 = convert_to_ltn(expression_2, predicates=predicates, functions=None, quantifier_impls={\"forall\" : \"pmean_error\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltn_imp.fuzzy_operators.aggregators import SatAgg\n",
    "\n",
    "sat_agg = SatAgg()\n",
    "\n",
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels in loader:\n",
    "        \n",
    "        pos =  data[torch.nonzero(labels)]\n",
    "        neg =  data[torch.nonzero(torch.logical_not(labels))]\n",
    "\n",
    "        # Compute satisfaction level\n",
    "        mean_sat += sat_agg(\n",
    "            rule_1( {\"x\" : pos} ),\n",
    "            rule_2( { \"x\" : neg })\n",
    "        )\n",
    "        \n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "def compute_accuracy(loader, model):\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels in loader:\n",
    "        predictions = model(data).detach().numpy()\n",
    "        predictions = np.where(predictions > 0.5, 1., 0.).flatten()\n",
    "        mean_accuracy += accuracy_score(labels, predictions)\n",
    "\n",
    "    return mean_accuracy / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.5003 | Train Sat 0.500 | Test Sat 0.499 | Train Acc 0.740 | Test Acc 0.720\n",
      "\n",
      "Positive 0.46363258361816406\n",
      "Negative 0.5394353270530701\n",
      "\n",
      " epoch 200 | loss 0.4669 | Train Sat 0.533 | Test Sat 0.506 | Train Acc 0.620 | Test Acc 0.620\n",
      "\n",
      "Positive 0.588340163230896\n",
      "Negative 0.4838448166847229\n",
      "\n",
      " epoch 400 | loss 0.3387 | Train Sat 0.663 | Test Sat 0.615 | Train Acc 0.840 | Test Acc 0.760\n",
      "\n",
      "Positive 0.6879205703735352\n",
      "Negative 0.6389533281326294\n",
      "\n",
      " epoch 600 | loss 0.1721 | Train Sat 0.828 | Test Sat 0.761 | Train Acc 0.960 | Test Acc 0.940\n",
      "\n",
      "Positive 0.8388997912406921\n",
      "Negative 0.8182598352432251\n",
      "\n",
      " epoch 800 | loss 0.1199 | Train Sat 0.880 | Test Sat 0.795 | Train Acc 1.000 | Test Acc 0.940\n",
      "\n",
      "Positive 0.8818762302398682\n",
      "Negative 0.8786745071411133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(801):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pos =  data[torch.nonzero(labels)]\n",
    "        neg =  data[torch.nonzero(torch.logical_not(labels))]\n",
    "        \n",
    "        # Compute satisfaction level\n",
    "        sat_agg_value = sat_agg(\n",
    "            rule_1( {\"x\" : pos} ),\n",
    "            rule_2( { \"x\" : neg })\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = 1.0 - sat_agg_value\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"\n",
    "              %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
    "                    compute_accuracy(train_loader, model), compute_accuracy(test_loader, model)))\n",
    "        \n",
    "        print()\n",
    "        print(f\"Positive {rule_1( {\"x\" : pos} )}\")\n",
    "        print(f\"Negative { rule_2( { \"x\" : neg })}\") \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ltn.Predicate(ModelA())\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels in loader:\n",
    "        \n",
    "        x_A = ltn.Variable(\"x_A\", data[torch.nonzero(labels)])  # positive examples\n",
    "        x_not_A = ltn.Variable(\"x_not_A\", data[torch.nonzero(torch.logical_not(labels))])  # negative examples\n",
    "\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_A, A(x_A)),\n",
    "            Forall(x_not_A, Not(A(x_not_A)))\n",
    "        )\n",
    "        \n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "def compute_accuracy(loader):\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels in loader:\n",
    "        predictions = A.model(data).detach().numpy()\n",
    "        predictions = np.where(predictions > 0.5, 1., 0.).flatten()\n",
    "        mean_accuracy += accuracy_score(labels, predictions)\n",
    "\n",
    "    return mean_accuracy / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.5056 | Train Sat 0.495 | Test Sat 0.494 | Train Acc 0.740 | Test Acc 0.720\n",
      "\n",
      "Positive LTNObject(value=tensor(0.4266, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "Negative LTNObject(value=tensor(0.5744, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "\n",
      " epoch 200 | loss 0.4644 | Train Sat 0.536 | Test Sat 0.507 | Train Acc 0.620 | Test Acc 0.600\n",
      "\n",
      "Positive LTNObject(value=tensor(0.5957, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "Negative LTNObject(value=tensor(0.4829, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "\n",
      " epoch 400 | loss 0.3713 | Train Sat 0.630 | Test Sat 0.588 | Train Acc 0.780 | Test Acc 0.800\n",
      "\n",
      "Positive LTNObject(value=tensor(0.6711, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "Negative LTNObject(value=tensor(0.5924, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "\n",
      " epoch 600 | loss 0.1745 | Train Sat 0.826 | Test Sat 0.753 | Train Acc 0.960 | Test Acc 0.920\n",
      "\n",
      "Positive LTNObject(value=tensor(0.8361, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "Negative LTNObject(value=tensor(0.8164, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "\n",
      " epoch 800 | loss 0.1180 | Train Sat 0.882 | Test Sat 0.786 | Train Acc 1.000 | Test Acc 0.940\n",
      "\n",
      "Positive LTNObject(value=tensor(0.8840, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "Negative LTNObject(value=tensor(0.8808, grad_fn=<RsubBackward1>), free_vars=[])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(A.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(801):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_A = ltn.Variable(\"x_A\", data[torch.nonzero(labels)]) # positive examples\n",
    "        x_not_A = ltn.Variable(\"x_not_A\", data[torch.nonzero(torch.logical_not(labels))]) # negative examples\n",
    "\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_A, A(x_A)),\n",
    "            Forall(x_not_A, Not(A(x_not_A)))\n",
    "        )\n",
    "\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"\n",
    "        %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
    "            compute_accuracy(train_loader), compute_accuracy(test_loader)))\n",
    "        \n",
    "        print()\n",
    "        print(f\"Positive { Forall(x_A, A(x_A)) }\")\n",
    "        print(f\"Negative { Forall(x_not_A, Not(A(x_not_A)))}\") \n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
